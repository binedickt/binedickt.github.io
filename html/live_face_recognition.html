<html>
    <head>
      <title>Live Face Recognition</title>
      <style>

        @media only screen and (max-width: 600px) {
          form {
            margin: 10px;
            padding: 10px;
          }
          form label {
            width: 100px;
            font-size: 14px;
          }
          form input[type="text"] {
            width: 100%;
          }
          p {
            font-size: 14px;
          }
        }
    
        /* Add some basic styling for the entire page */
        body {
          font-family: Arial, sans-serif;
          background-color: #333;
          color: #f2f2f2;
        }
        /* Style the form elements */


        /* Style the submit button */
        input[type="submit"] {
          background-color: #4CAF50;
          color: white;
          padding: 14px 20px;
          margin: 8px 0;
          border: none;
          cursor: pointer;
        }
        h1 {text-align: center;}
      </style>
      <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
      <script defer src="https://pyscript.net/latest/pyscript.js"></script>
    </head>

  <body>
    <h1>Live Face Recognition</h1>
    <!-- <video id="myVidPlayer" controls muted autoplay></video> -->
    <!-- <div class="mycanvas">
      <h6>Captured snapshot</h6>
      <canvas></canvas>
    </div> -->
    <input type="submit" value="start" id="btn-load" class="py-button" py-click="face_recog()">
    <!-- <script>
      var canvas = document.querySelector("canvas");
      var context = canvas.getContext("2d");
      const video = document.querySelector('#myVidPlayer');
      
      //w-width,h-height
      var w, h;
      canvas.style.display = "none";
      
      //new

      window.navigator.mediaDevices.getUserMedia({ video: true, audio: true })
          .then(stream => {
              video.srcObject = stream;
              video.onloadedmetadata = (e) => {
                  video.play();
                  
                  //new
                  w = video.videoWidth;
                  h = video.videoHeight
                  
                  canvas.width = w;
                  canvas.height = h;
              };
          })
          .catch(error => {
              alert('You have to enable the mike and the camera');
          });

    </script> -->

    <p><span id="plot"></span></p>

    <py-config type="toml">
        
        packages = ["opencv-python", "matplotlib"]

        [[fetch]]
        files = ["./files/haarcascade_frontalface_default.xml"]
        
    </py-config>
    <py-script>
    def face_recog():
      import cv2
      from matplotlib import pyplot as plt
      cap = cv2.VideoCapture(0)
      faceCascade = cv2.CascadeClassifier("./files/haarcascade_frontalface_default.xml")
      while(True):
          # Capture frame-by-frame
          ret, frame = cap.read()
          # Our operations on the frame come here
          # gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
          # Detect faces in the image
          # faces = faceCascade.detectMultiScale(
          #    gray,
          #    scaleFactor=1.1,
          #    minNeighbors=5,
          #    minSize=(30, 30)
          #    #flags = cv2.CV_HAAR_SCALE_IMAGE
          #)
          # print("Found {0} faces!".format(len(faces)))
          # Draw a rectangle around the faces
          #for (x, y, w, h) in faces:
          #    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
          # Display the resulting frame
          
          plt.imshow(frame)
          plt.show()
          time.sleep(1)
          plt.close()

      # When everything done, release the capture
      # cap.release()
      # cv2.destroyAllWindows()
      

    </py-script>
  </body>
</html>